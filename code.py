# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FDqZJZvgiMvxJFWZbhyVcdbHCAMDBb2_

# Student ID: 2201010

Let's install all require libraries. For example, `transformers`
"""

!pip install transformers

"""Let's import all require libraries. 
For example, `numpy`
"""

import numpy as np
import os
import pandas as pd


import numpy
import io 
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, f1_score, accuracy_score

import pickle 
import re

from sklearn.svm import SVC

"""**Let's put your student id as a variable, that you will use different places**"""

student_id = 2201010 # Note this is an interger and you need to input your id
vectorizer_filename = 'vectorizer.sav'

"""Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"""

# set same seeds for all libraries

#numpy seed
np.random.seed(2201010)

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Add your code to initialize GDrive and data and models paths

# TODO: Fill in the Google Drive path where you uploaded the assignment, data and code
# Example: If your student_id is 1234567 then your directory will be './CE807/Assignment2/1234567/' 

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./CE807/Assignment2/',str(student_id)) # Make sure to update with your student_id and student_id is an integer
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv')
print('Train file: ', train_file)

MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1') # Model 1 directory
print('Model 1 directory: ', MODEL_1_DIRECTORY)

MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25') # Model 1 trained using 25% of train data directory
print('Model 1 directory with 25% data: ', MODEL_1_25_DIRECTORY)

model_1_25_output_test_file = os.path.join(MODEL_1_25_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data 
print('Output file name using model 1 using 25% of train data: ',model_1_25_output_test_file)
# You need to do this for both models and all data sizes 

###############################################################

MODEL_1_50_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'50') # Model 1 trained using 50% of train data directory
print('Model 1 directory with 50% data: ', MODEL_1_50_DIRECTORY)

model_1_50_output_test_file = os.path.join(MODEL_1_50_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 50% of train data 
print('Output file name using model 1 using 50% of train data: ',model_1_50_output_test_file)

MODEL_1_75_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'75') # Model 1 trained using 75% of train data directory
print('Model 1 directory with 75% data: ', MODEL_1_75_DIRECTORY)

model_1_75_output_test_file = os.path.join(MODEL_1_75_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 75% of train data 
print('Output file name using model 1 using 75% of train data: ',model_1_75_output_test_file)

MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100') # Model 1 trained using 100% of train data directory
print('Model 1 directory with 100% data: ', MODEL_1_100_DIRECTORY)

model_1_100_output_test_file = os.path.join(MODEL_1_100_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 100% of train data 
print('Output file name using model 1 using 100% of train data: ',model_1_100_output_test_file)

# For Model 2

MODEL_2_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '2') # Model 2 directory
print('Model 2 directory: ', MODEL_2_DIRECTORY)

MODEL_2_25_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'25') # Model 2 trained using 25% of train data directory
print('Model 2 directory with 25% data: ', MODEL_2_25_DIRECTORY)

model_2_25_output_test_file = os.path.join(MODEL_2_25_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 25% of train data 
print('Output file name using model 2 using 25% of train data: ',model_2_25_output_test_file)

MODEL_2_50_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'50') # Model 2 trained using 50% of train data directory
print('Model 2 directory with 50% data: ', MODEL_2_50_DIRECTORY)

model_2_50_output_test_file = os.path.join(MODEL_2_50_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 50% of train data 
print('Output file name using model 2 using 50% of train data: ',model_2_50_output_test_file)

MODEL_2_75_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'75') # Model 2 trained using 75% of train data directory
print('Model 2 directory with 75% data: ', MODEL_2_75_DIRECTORY)

model_2_75_output_test_file = os.path.join(MODEL_2_75_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 75% of train data 
print('Output file name using model 2 using 75% of train data: ',model_2_75_output_test_file)

MODEL_2_100_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'100') # Model 2 trained using 100% of train data directory
print('Model 2 directory with 100% data: ', MODEL_2_100_DIRECTORY)

model_2_100_output_test_file = os.path.join(MODEL_2_100_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 100% of train data 
print('Output file name using model 2 using 100% of train data: ',model_2_100_output_test_file)

"""Let's see train file"""

train = pd.read_csv(train_file)
train.head()
len(train)

from sklearn.model_selection import train_test_split
import pandas as pd

# Split the data into training and testing sets using stratification
TRAIN_75, TEST_75 =  train_test_split(train, train_size=0.75, shuffle = True, stratify = train['label'])

TRAIN_75=pd.DataFrame(TRAIN_75)

len(TRAIN_75)

TRAIN_75.to_csv(os.path.join(GOOGLE_DRIVE_PATH,'train_75.csv'))

# Split the data into training and testing sets using stratification
TRAIN_50, TEST_50 =  train_test_split(TRAIN_75, train_size=0.50, shuffle = True, stratify = TRAIN_75['label'])
TRAIN_25, TEST_25 =  train_test_split(TRAIN_50, train_size=0.25, shuffle = True, stratify = TRAIN_50['label'])

TRAIN_50=pd.DataFrame(TRAIN_50)
TRAIN_25=pd.DataFrame(TRAIN_25)

TRAIN_50.to_csv(os.path.join(GOOGLE_DRIVE_PATH,'train_50.csv'))
TRAIN_25.to_csv(os.path.join(GOOGLE_DRIVE_PATH,'train_25.csv'))

train_25_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_25.csv')
print('Train 25% file: ', train_25_file)

train_50_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_50.csv')
print('Train 50% file: ', train_50_file)

train_75_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_75.csv')
print('Train 75% file: ', train_75_file)

valid_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')
print('Validation File: ', valid_file)

test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')
print('Test file: ', test_file)

"""We are going to use different performance matrics like Accuracy, Recall (macro), Precision (macro), F1 (macro) and Confusion Matrix for the performance evaluation. We will print all the matrics and display Confusion Matrix with proper X & Y axis labels"""

def compute_performance(y_true, y_pred):

  """
    prints different performance matrics like  Accuracy, Recall (macro), Precision (macro), and F1 (macro).
    This also display Confusion Matrix with proper X & Y axis labels.
    Also, returns F1 score

    Args:
        y_true: numpy array or list
        y_pred: numpy array or list
        

    Returns:
        float
    """

  f1score=f1_score(y_true, y_pred, average='macro')
  acc = accuracy_score(y_true, y_pred)
    
  print('F1 Score(macro): ', f1score)
  print('Accuracy: ', acc)

  return f1score

"""# Method 1 Start


"""

def preprocess_data(data, vectorizer_count=None, split='test'):
  if split == 'train':
      data['tweet'] = data['tweet'].apply(lambda x: re.sub('@[^\s]+','',x))
      data["tweet"] = data["tweet"].apply(lambda x: re.sub(r"http\S+", "", x))
      data["tweet"] = data["tweet"].apply(lambda x: re.sub(r"#\S+", "", x))

      vectorizer_count = CountVectorizer(stop_words='english',max_features=5000) 
      values = vectorizer_count.fit_transform(data['tweet'].values) #TODO: This is the best way to do this, because you need to use same vectorization menthod
  else:
      values = vectorizer_count.transform(data['tweet'].values)

  if split == 'train':
      return values, vectorizer_count
  else:
      return values

def model_SVM(text_vector,label):

    print('Let\'s start training the model')
    classifier = SVC(kernel='linear')
    classifier.fit(text_vector, label)

    return classifier

def save_M1(model, vectorizer, model_dir):
    # save the model to disk
    model_file = os.path.join(model_dir, 'model.sav')
    pickle.dump(model, open(model_file, 'wb'))

    print('model is saved to ', model_file)

    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav') 
    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))

    print('vectorizer is saved to ', vectorizer_file)

    return model_file, vectorizer_file

def load_M1(model_file, vectorizer_file):
    # load model and vectorizer from disk

    model = pickle.load(open(model_file, 'rb'))

    print('model loaded from ', model_file)

    vectorizer = pickle.load(open(vectorizer_file, 'rb'))

    print('loading Vectorizer from ', vectorizer_file)


    return model, vectorizer

"""## Training Method 1 Code
 
"""

def train_method1(train_file, val_file, model_dir):
    """
     Takes train_file, val_file and model_dir as input.
     It trained on the train_file datapoints, and validate on the val_file datapoints.
     While training and validating, it print different evaluataion metrics and losses, wheverever necessary.
     After finishing the training, it saved the best model in the model_dir.

     ADD Other arguments, if needed.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory
    
    """
    train = pd.read_csv(train_file)
    val_df = pd.read_csv(val_file)

    train_label = train['label']
    val_label = val_df['label']

    train_values, vectorizer_count = preprocess_data(train, split='train') 
    val_values= preprocess_data(val_df,vectorizer_count)

    model = model_SVM(train_values,train_label)

    model_file, vectorizer_file = save_M1(model, vectorizer_count, model_dir)

    train_pred_label = model.predict(train_values)
    val_pred_label = model.predict(val_values)

    print('Train Split')
    train_f1_score = compute_performance(train_label, train_pred_label)

    print('Validation Split')
    val_f1_score = compute_performance(val_label, val_pred_label)

    return model_file, vectorizer_file, val_f1_score

print('Train using 100% of data')
model_100_file, vectorizer_100_file, val_f1_score_100 = train_method1(train_file, valid_file, MODEL_1_100_DIRECTORY)

print('Train using 75% of data')
model_75_file, vectorizer_75_file, val_f1_score_75 = train_method1(train_75_file, valid_file, MODEL_1_75_DIRECTORY)

print('Train using 50% of data')
model_50_file, vectorizer_50_file, val_f1_score_50 = train_method1(train_50_file, valid_file, MODEL_1_50_DIRECTORY)

print('Train using 25% of data')
model_25_file, vectorizer_25_file, val_f1_score_25 = train_method1(train_25_file, valid_file, MODEL_1_25_DIRECTORY)

"""## Testing Method 1 Code

"""

def test_method1(test_file, model_file, output_dir):
    """
     take test_file, model_file and output_dir as input.
     It loads model and test of the examples in the test_file.
     It prints different evaluation metrics, and saves the output in output directory

     ADD Other arguments, if needed

    Args:
        test_file: Test file name
        model_file: Model file name
        vectorizer_file: Vectorizer file name
        output_dir: Output Directory
    
    """

    test_df = pd.read_csv(test_file)
    
    test_label = test_df['label']

    model, vectorizer = load_M1(model_file, os.path.join(output_dir, vectorizer_filename)) 

    test_values= preprocess_data(test_df,vectorizer)

    test_pred_label = model.predict(test_values)

    test_df['out_label']  = test_pred_label # Note how this is saved 

    test_f1_score = compute_performance(test_label, test_pred_label)

    out_file = os.path.join(output_dir, 'output_test.csv')

    print('Saving model output to', out_file)
    test_df.to_csv(out_file)
    
    return test_f1_score

print('Testing model which is trained on 100% data')
test_f1_score_100 = test_method1(test_file, model_100_file, MODEL_1_100_DIRECTORY)
len(test_file)
len(valid_file)

print('Testing model which is trained on 75% data')
test_f1_score_75 = test_method1(test_file, model_75_file, MODEL_1_75_DIRECTORY)

print('Testing model which is trained on 50% data')
test_f1_score_50 = test_method1(test_file, model_50_file, MODEL_1_50_DIRECTORY)

print('Testing model which is trained on 25% data')
test_f1_score_25 = test_method1(test_file, model_25_file, MODEL_1_25_DIRECTORY)

import matplotlib.pyplot as plt

# Data for the graph
x = [25, 50, 75, 100]
y1 = [test_f1_score_25, test_f1_score_50, test_f1_score_75, test_f1_score_100]
y2 = [val_f1_score_25, val_f1_score_50, val_f1_score_75, val_f1_score_100]

# Labels for the x-axis
labels = ['25%', '50%', '75%', '100%']

# Create a figure and two axes objects
fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

# Plot the line graphs with different colors and markers
ax1.plot(x, y1, marker='o', linestyle='solid', linewidth=2, color='orange')
ax2.plot(x, y2, marker='s', linestyle='solid', linewidth=2, color='blue')

# Add x-axis labels
ax1.set_xticks(x)
ax1.set_xticklabels(labels)

# Add labels to the axes
ax1.set_xlabel('% of Data Used for Testing', fontsize=14, color='green', labelpad=10)
ax1.set_ylabel('Test F1 Score', fontsize=14, color='orange', labelpad=10)
ax2.set_ylabel('valid F1 Score', fontsize=14, color='blue', labelpad=10)

# Add a title to the graph
plt.title('Testing and validating Results - Model 1', fontsize=16, color='#4d4d4d', pad=15)

# Change the background color
plt.rcParams['axes.facecolor'] = '#f5f5f5'

# Add grid lines
ax1.grid(color='#d9d9d9', linestyle='--', linewidth=1, alpha=0.7)

# Customize the tick labels
ax1.tick_params(axis='both', which='major', labelsize=12, color='#4d4d4d', pad=10)
ax2.tick_params(axis='both', which='major', labelsize=12, color='#4d4d4d', pad=10)

# Add a border to the graph
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
ax1.spines['bottom'].set_color('#d9d9d9')
ax1.spines['left'].set_color('orange')
ax2.spines['top'].set_visible(False)
ax2.spines['left'].set_visible(False)
ax2.spines['bottom'].set_color('#d9d9d9')
ax2.spines['right'].set_color('blue')

# Add legends to the graph
ax1.legend(['Test F1 Score'], loc='best')
ax2.legend(['valid F1 Score'], loc='upper center')



# Display the graph
plt.show()

"""## Method 1 End

# Method 2 Start
"""

from sklearn.linear_model import LogisticRegression

def train_LR(text_vector, label):
    print('Let\'s start training the model')
    model = LogisticRegression()
    model.fit(text_vector, label)
    return model

def save_model2(model, vectorizer, model_dir):
    # save the model to disk
    model_file = os.path.join(model_dir, 'model.sav')
    pickle.dump(model, open(model_file, 'wb'))

    print('Saved model to ', model_file)

    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav') 
    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))

    print('Saved Vectorizer to ', vectorizer_file)

    return model_file, vectorizer_file

def load_model2(model_file, vectorizer_file):
    # load model and vectorizer from disk

    model = pickle.load(open(model_file, 'rb'))

    print('Loaded model from ', model_file)

    vectorizer = pickle.load(open(vectorizer_file, 'rb'))

    print('Loaded Vectorizer from ', vectorizer_file)


    return model, vectorizer

"""## Training Method 2 Code

"""

def train_method2(train_file, val_file, model_dir):
    """
     Takes train_file, val_file and model_dir as input.
     It trained on the train_file datapoints, and validate on the val_file datapoints.
     While training and validating, it print different evaluataion metrics and losses, wheverever necessary.
     After finishing the training, it saved the best model in the model_dir.

     ADD Other arguments, if needed.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory
    
    """
    train = pd.read_csv(train_file)
    val_df = pd.read_csv(val_file)

    train_label = train['label']
    val_label = val_df['label']

    train_values, vectorizer_count = preprocess_data(train, split='train') 
    val_values= preprocess_data(val_df,vectorizer_count)

    model = train_LR(train_values,train_label)

    model_file, vectorizer_file = save_model2(model, vectorizer_count, model_dir)

    train_pred_label = model.predict(train_values)
    val_pred_label = model.predict(val_values)

    print('Train Split')
    train_f1_score = compute_performance(train_label, train_pred_label)

    print('Validation Split')
    val_f1_score = compute_performance(val_label, val_pred_label)

    return model_file, vectorizer_file, val_f1_score

print('Train using 100% of data')
model_100_file, vectorizer_100_file, val_f1_score_100 = train_method2(train_file, valid_file, MODEL_2_100_DIRECTORY)

print('Train using 75% of data')
model_75_file, vectorizer_75_file, val_f1_score_75 = train_method2(train_75_file, valid_file, MODEL_2_75_DIRECTORY)

print('Train using 50% of data')
model_50_file, vectorizer_50_file, val_f1_score_50 = train_method2(train_50_file, valid_file, MODEL_2_50_DIRECTORY)

print('Train using 25% of data')
model_25_file, vectorizer_25_file, val_f1_score_25 = train_method2(train_25_file, valid_file, MODEL_2_25_DIRECTORY)

"""## Testing Method 2 Code

"""

def test_method2(test_file, model_file, output_dir):
    """
     take test_file, model_file and output_dir as input.
     It loads model and test of the examples in the test_file.
     It prints different evaluation metrics, and saves the output in output directory

     ADD Other arguments, if needed

    Args:
        test_file: Test file name
        model_file: Model file name
        vectorizer_file: Vectorizer file name
        output_dir: Output Directory
    
    """

    test_df = pd.read_csv(test_file)
    
    test_label = test_df['label']

    model, vectorizer = load_model2(model_file, os.path.join(output_dir, vectorizer_filename)) 

    test_values= preprocess_data(test_df,vectorizer)

    test_pred_label = model.predict(test_values)

    test_df['out_label']  = test_pred_label # Note how this is saved 

    test_f1_score = compute_performance(test_label, test_pred_label)

    out_file = os.path.join(output_dir, 'output_test.csv')

    print('Saving model output to', out_file)
    test_df.to_csv(out_file)
    
    return test_f1_score

print('Testing using model trained on 100% data')
test_f1_score_100 = test_method2(test_file, model_100_file, MODEL_2_100_DIRECTORY)

print('Testing using model trained on 75% data')
test_f1_score_75 = test_method2(test_file, model_75_file, MODEL_2_75_DIRECTORY)

print('Testing using model trained on 50% data')
test_f1_score_50 = test_method2(test_file, model_50_file, MODEL_2_50_DIRECTORY)

print('Testing using model trained on 25% data')
test_f1_score_25 = test_method2(test_file, model_25_file, MODEL_2_25_DIRECTORY)

import matplotlib.pyplot as plt

# Data for the graph
x = [25, 50, 75, 100]
y1 = [test_f1_score_25, test_f1_score_50, test_f1_score_75, test_f1_score_100]
y2 = [val_f1_score_25, val_f1_score_50, val_f1_score_75, val_f1_score_100]

# Labels for the x-axis
labels = ['25%', '50%', '75%', '100%']

# Create a figure and two axes objects
fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

# Plot the line graphs with different colors and markers
ax1.plot(x, y1, marker='o', linestyle='solid', linewidth=2, color='orange')
ax2.plot(x, y2, marker='s', linestyle='solid', linewidth=2, color='blue')

# Add x-axis labels
ax1.set_xticks(x)
ax1.set_xticklabels(labels)

# Add labels to the axes
ax1.set_xlabel('% of Data Used for Testing', fontsize=14, color='green', labelpad=10)
ax1.set_ylabel('Test F1 Score', fontsize=14, color='orange', labelpad=10)
ax2.set_ylabel('valid F1 Score', fontsize=14, color='blue', labelpad=10)

# Add a title to the graph
plt.title('Testing and validating Results - Model 2', fontsize=16, color='#4d4d4d', pad=15)

# Change the background color
plt.rcParams['axes.facecolor'] = '#f5f5f5'

# Add grid lines
ax1.grid(color='#d9d9d9', linestyle='--', linewidth=1, alpha=0.7)

# Customize the tick labels
ax1.tick_params(axis='both', which='major', labelsize=12, color='#4d4d4d', pad=10)
ax2.tick_params(axis='both', which='major', labelsize=12, color='#4d4d4d', pad=10)

# Add a border to the graph
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
ax1.spines['bottom'].set_color('#d9d9d9')
ax1.spines['left'].set_color('orange')
ax2.spines['top'].set_visible(False)
ax2.spines['left'].set_visible(False)
ax2.spines['bottom'].set_color('#d9d9d9')
ax2.spines['right'].set_color('blue')

# Add legends to the graph
ax1.legend(['Test F1 Score'], loc='lower right')
ax2.legend(['valid F1 Score'], loc='lower center')


# Display the graph
plt.show()

"""## Method 2 End

"""